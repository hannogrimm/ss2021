{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oaykw3Gwi-CO"
   },
   "source": [
    "# Overfitting Test Notebook\n",
    "\n",
    "This notebook is designed as a showcase for overfitting. \n",
    "Overfitting can happen if you try to explain data with some kind of model (e.g., a function). Data - especially if generated by physical sensors - is usually noisy. This means, in addition to the valuable information in the data there is a bunch of garbage information that is not valuable to us. \n",
    "\n",
    "To illustrate this, imagine a microphone. It picks up the voice of the person speaking into it, but it will also pick up other noises around the microphone like typing on a keyboard or the fan of your computer. And it may also pick up static noise. If we wanted to create a transcript of the speaker, we would like to focus only on the spoken words. All other sounds are noise that we want to ignore. \n",
    "\n",
    "In this notebook we will look at the problem of overfitting specifically for machine learning. But you should be aware that the problem also exists outside of machine learning. Deep knowledge of machine learning will not be required for this notebook. Everything you need will be explained in the accompanying text.\n",
    "\n",
    "Before we start let's do some coding busywork so we have less clutter later on. First, let's import everyting we need later:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlWBS19Ui-CU"
   },
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "# general imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# sklearn utility\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# machine learning models\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn import gaussian_process\n",
    "from sklearn import cross_decomposition\n",
    "from sklearn import tree \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwsLMG4Hi-CV"
   },
   "source": [
    "And second, let's define some helper functions that we can use later to make the code more readible:\n",
    "You don't need to understand them to understand this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NupBIrC_i-CV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def f_pure(input):\n",
    "  \"\"\"Produces data samples without noise\"\"\"\n",
    "  return math.sin(input*10) + math.cos(1.8*input*10)\n",
    "\n",
    "\n",
    "def f_noisy(input):\n",
    "  \"\"\"Function for producing noisy data samples. It calls f_pure and add noise\"\"\"\n",
    "  return f_pure(input) + np.random.uniform(-0.3,0.3)\n",
    "\n",
    "\n",
    "def samples(number):\n",
    "    \"\"\" creates a number of samples. \n",
    "        Returns a data frame for the input and a list for output (i.e., it's compatible with the machine learning libraries)\"\"\"\n",
    "    data = {\"key\":np.random.rand(number)}\n",
    "    x_train = pd.DataFrame(data=data)\n",
    "\n",
    "    y_train = [f_noisy(x) for x in data[\"key\"]]\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def plot_data(x_train, y_train, x_val = None, y_val = None, show_functions = False, model = None):\n",
    "    \"\"\" plots our data and additional information as configured by input parameters \"\"\"\n",
    "    # create plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # plot samples \n",
    "    ax.scatter(x_train, y_train)\n",
    "\n",
    "    # plot test samples if needed\n",
    "    if x_val is not None and y_val is not None:\n",
    "        ax.scatter(x_val,y_val,c='g')\n",
    "\n",
    "    # plot the function f_pure if needed\n",
    "    if show_functions:\n",
    "        f_x = np.arange(0,1,0.001)\n",
    "        f_y = [f_pure(x) for x in f_x]\n",
    "        plt.plot(f_x, f_y,'y')\n",
    "\n",
    "    # plot the function f_pure if needed\n",
    "    if model is not None:\n",
    "        f_x = np.arange(0,1,0.001)\n",
    "        df2 = pd.DataFrame(data={\"key\":f_x})\n",
    "        plt.plot(f_x, model.predict(df2),'r') \n",
    "\n",
    "    # show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcFkUrR1i-CW"
   },
   "source": [
    "## The Data Set\n",
    "\n",
    "Before looking at overfitting, we will first need some data to overfit to. Let's make a small dataset with one featue and one label that can easily be plotted for illustration. You can imagine it as the input and output of a one-dimensional function (i.e., a function that has one input value and one output value).\n",
    "\n",
    "Normally, this is where you would load a dataset. Here, we're not too concerned with what the data represents and will just generate a fake dataset that fits our requirements. This dataset is generated by using the function f(x) = sin(x/10)+ cos(1.8*x/10) and adding some noise by adding a random number between 0.3 and -0.3. \n",
    "\n",
    "In the following cell we calculate 100 random samples of this function and store them into:\n",
    "- x_train: the input of the function\n",
    "- y_train: the output (or f(x) value) of the function with noise\n",
    "\n",
    "If you run the cell you see a graphical representation of the points we've created. \n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. Based on these points, can you draw the original function? Keep in mind that the points contain noise.\n",
    "2. Do you see any patterns in this drawing that are not part of the function but are ceated by noise?\n",
    "3. If you run the cell again, you see 100 different random points. How did this change the patterns you've identified in question 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "R6Z2dOMEi-CW",
    "outputId": "6824414d-3772-49e9-8dea-8accc3137983"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train = samples(100)\n",
    "\n",
    "plot_data(x_train[\"key\"],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG-e9WdQi-CX"
   },
   "source": [
    "## Machine learning\n",
    "\n",
    "In machine learning our main task is to select a model that we then learn to fit the data.\n",
    "You can imagine a \"model\" as defining the shape of a function (e.g., a straight line, a parabola, etc). The learning process will then try to find a function that conforms to this shape and is as close as possible to the data points.\n",
    "\n",
    "An example: Let's say we suspect a linear function to be a good explanation of our data. We could then try to do linear regression and find a linear function of the form f(x) = ax + b that is as close as possible to our data points. The learning process would be responsible for finding the best values for a and b.\n",
    "\n",
    "And this, on it's core, is machine learning. It consists of three steps:\n",
    "\n",
    "1. create a model\n",
    "2. learn the model \n",
    "3. visualize the results \n",
    "\n",
    "And this is why machine learning can often be implemented in a handful lines of code. Check out the cell below to see how the three steps are implemented. It tries to fit a linear function to our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "sAnXGbpyHWjD",
    "outputId": "74b44be7-8052-4fe8-e8ca-8fa76343a8c3"
   },
   "outputs": [],
   "source": [
    "# 1. create a model\r\n",
    "model = linear_model.LinearRegression()\r\n",
    "\r\n",
    "# 2. learn the model \r\n",
    "model.fit(x_train, y_train)\r\n",
    "\r\n",
    "# 3. visualize the results \r\n",
    "print(\"mean squared error: \" + str(mean_squared_error(y_train, model.predict(x_train))))\r\n",
    "plot_data(x_train[\"key\"],y_train,model= model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhDLdrf7HplL"
   },
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "Just from looking at the output of the cell it is obvious that the linear model is not a good model to explain our data. It's pretty far away from our data points.\r\n",
    "\r\n",
    "To capture this numericaly, we use an error function.  Here, we use the mean square error, which represents the average squared distance between our data samples $x_i$ and the line $\\hat{x}_i$.\r\n",
    "$$\r\n",
    "MSE = \\frac{\\sum _{i\\in \\{1...N\\}} (x_i - \\hat{x} _i)^2}{N}\r\n",
    "$$\r\n",
    "If our function perfectly captures all data points the MSE is 0. In our case the MSE is somewhere around 0.9 (depending on your random samples). Considering the overall variation of our points is between -2 and 2, this is pretty bad.\r\n",
    "\r\n",
    "And this is where your tasks come in. Let's find a better model to explain our data.\r\n",
    "\r\n",
    "\r\n",
    "### Task 1 - Model selection\r\n",
    "\r\n",
    "In Step one of the cell below, there are several machine learning models that you can try. Your task is to try them and determine which one has the lowest mean square error. Just comment the respective lines in / out to test them.\r\n",
    "\r\n",
    "Once you have found the best model, please copy it into this cell. You'll need it later, so you don't want to loose it when doing the next task.\r\n",
    "\r\n",
    "Answer: \\<copy of your line of code here\\>\r\n",
    "\r\n",
    "\r\n",
    "### Reflection Question:\r\n",
    "\r\n",
    "1. Do you think the model with the best mean square error is also best suited to explain the data?\r\n",
    "2. Which one would do you think is better?\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "JiWAp6VEi-Ca",
    "outputId": "35473806-24bd-4802-8794-3e80585df9d8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. create a model\n",
    "model = svm.SVR()\n",
    "#model = neighbors.KNeighborsRegressor()\n",
    "#model = gaussian_process.GaussianProcessRegressor()\n",
    "#model = tree.DecisionTreeRegressor()\n",
    "\n",
    "# 2. learn the model \n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 3. visualize the results \n",
    "print(\"mean squared error: \" + str(mean_squared_error(y_train, model.predict(x_train))))\n",
    "plot_data(x_train[\"key\"],y_train,model= model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xIAZBktKZ54"
   },
   "source": [
    "\r\n",
    "### Task 2 - Hyper Paraemter Selection\r\n",
    "\r\n",
    "In addition to selecting models, a machine learning expert can select hyper parameters. These are parameters that can be used to tune the respective models to tweak their results. In the cell below  you can see them in the respective constructing functions. E.g., SVR(C=1) has one hyper paraemter C which has been set to value 1.\r\n",
    "\r\n",
    "Your next task is to play around with these hyper parameters. Your goal is to change the model so it learns as much as possible about the original function and as little as possible about the noise. This means you'll have a tradeoff between the mean square error and the shape of the function. For now just use your eye measure to determine which function you think is closest to the original function.\r\n",
    "\r\n",
    "\r\n",
    "All parameters expect positive whole numbers (in case of alpha you can change the 10 in \"1e-10\" to change how small this number should be).\r\n",
    "\r\n",
    "Again, let's keep track of the answer:\r\n",
    "\r\n",
    "Answer: \\<copy of your line of code here\\>\r\n",
    "\r\n",
    "### Reflection Question:\r\n",
    "\r\n",
    "1. How hard is it for you to judge whether a given function is actually the right one?\r\n",
    "2. How would you define an automatic process to judge whether the function is the right one?\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "HgQJTZXJRiHY",
    "outputId": "4cfacf66-44bf-4bbd-bbfc-d92b7843eae7"
   },
   "outputs": [],
   "source": [
    "# 1. create a model\r\n",
    "model = svm.SVR(C=1)\r\n",
    "#model = neighbors.KNeighborsRegressor(n_neighbors = 5)\r\n",
    "#model = gaussian_process.GaussianProcessRegressor(alpha=1e-10)\r\n",
    "#model = tree.DecisionTreeRegressor(max_depth=20,min_samples_split=2)\r\n",
    "\r\n",
    "# 2. learn the model \r\n",
    "model.fit(x_train, y_train)\r\n",
    "\r\n",
    "# 3. visualize the results \r\n",
    "print(\"mean squared error: \" + str(mean_squared_error(y_train, model.predict(x_train))))\r\n",
    "plot_data(x_train[\"key\"],y_train,model= model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "overfitting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
